{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plotly in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (6.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from plotly) (1.33.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from plotly) (24.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tbila\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly pandas\n",
    "\n",
    "# üìö Imports\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# üìÇ Ensure output folder exists\n",
    "os.makedirs(\"visuals\", exist_ok=True)\n",
    "\n",
    "# üì• Load CSVs\n",
    "fp16 = pd.read_csv(\"data/evaluation_results_FP16.csv\")\n",
    "int8 = pd.read_csv(\"data/evaluation_results_INT8.csv\")\n",
    "int4 = pd.read_csv(\"data/evaluation_results_INT4.csv\")\n",
    "\n",
    "# üè∑Ô∏è Add precision column\n",
    "fp16[\"Precision\"] = \"FP16\"\n",
    "int8[\"Precision\"] = \"INT8\"\n",
    "int4[\"Precision\"] = \"INT4\"\n",
    "\n",
    "# üîó Combine into one DataFrame\n",
    "combined = pd.concat([fp16, int8, int4], ignore_index=True)\n",
    "\n",
    "# ‚úÖ Ensure numeric columns\n",
    "combined[\"BLEU Score\"] = pd.to_numeric(combined[\"BLEU Score\"], errors=\"coerce\")\n",
    "combined[\"Latency (ms)\"] = pd.to_numeric(combined[\"Latency (ms)\"], errors=\"coerce\")\n",
    "\n",
    "# üìå Create a unique variant ID\n",
    "combined[\"variant_id\"] = combined[\"Model\"] + \"_\" + combined[\"Precision\"]\n",
    "\n",
    "# ========== 1Ô∏è‚É£ FP16 BLEU Score Comparison ==========\n",
    "fig1 = px.bar(fp16, x=\"Model\", y=\"BLEU Score\", color=\"Model\",\n",
    "              title=\"1Ô∏è‚É£ BLEU Score Comparison Within FP16\",\n",
    "              labels={\"BLEU Score\": \"BLEU Score\", \"Model\": \"Model\"})\n",
    "\n",
    "fig1.update_layout(xaxis_tickangle=-45)\n",
    "fig1.write_html(\"visuals/interactive_FP16_bleu.html\")\n",
    "\n",
    "# ========== 2Ô∏è‚É£ INT8 BLEU Score Comparison ==========\n",
    "fig2 = px.bar(int8, x=\"Model\", y=\"BLEU Score\", color=\"Model\",\n",
    "              title=\"2Ô∏è‚É£ BLEU Score Comparison Within INT8\",\n",
    "              labels={\"BLEU Score\": \"BLEU Score\", \"Model\": \"Model\"})\n",
    "\n",
    "fig2.update_layout(xaxis_tickangle=-45)\n",
    "fig2.write_html(\"visuals/interactive_INT8_bleu.html\")\n",
    "\n",
    "# ========== 3Ô∏è‚É£ INT4 BLEU Score Comparison ==========\n",
    "fig3 = px.bar(int4, x=\"Model\", y=\"BLEU Score\", color=\"Model\",\n",
    "              title=\"3Ô∏è‚É£ BLEU Score Comparison Within INT4\",\n",
    "              labels={\"BLEU Score\": \"BLEU Score\", \"Model\": \"Model\"})\n",
    "\n",
    "fig3.update_layout(xaxis_tickangle=-45)\n",
    "fig3.write_html(\"visuals/interactive_INT4_bleu.html\")\n",
    "\n",
    "# ========== 4Ô∏è‚É£ Average BLEU & Latency Across Precisions ==========\n",
    "avg_metrics = combined.groupby(\"Precision\")[[\"BLEU Score\", \"Latency (ms)\"]].mean().reset_index()\n",
    "avg_melted = avg_metrics.melt(id_vars=\"Precision\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "fig4 = px.bar(avg_melted, x=\"Metric\", y=\"Value\", color=\"Precision\", barmode=\"group\",\n",
    "              title=\"4Ô∏è‚É£ Average BLEU and Latency Across Precisions\",\n",
    "              labels={\"Value\": \"Average Value\", \"Metric\": \"Metric\"})\n",
    "\n",
    "fig4.write_html(\"visuals/interactive_avg_metrics.html\")\n",
    "\n",
    "# ========== 5Ô∏è‚É£ BLEU Comparison Across All Model Variants ==========\n",
    "fig5 = px.bar(combined, x=\"variant_id\", y=\"BLEU Score\", color=\"Precision\",\n",
    "              title=\"5Ô∏è‚É£ BLEU Score Comparison Across All Model Variants\",\n",
    "              labels={\"BLEU Score\": \"BLEU Score\", \"variant_id\": \"Model + Precision\"})\n",
    "\n",
    "fig5.update_layout(xaxis_tickangle=-90)\n",
    "fig5.write_html(\"visuals/interactive_all_variants_bleu.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these new visualizations to your existing notebook\n",
    "\n",
    "# ========== 6Ô∏è‚É£ Performance vs. Efficiency Scatter Plot ==========\n",
    "# This shows the trade-off between BLEU score and latency\n",
    "fig6 = px.scatter(combined, x=\"Latency (ms)\", y=\"BLEU Score\", \n",
    "                  color=\"Precision\", symbol=\"Model\", size=\"BLEU Score\",\n",
    "                  hover_data=[\"Model\", \"Precision\", \"BLEU Score\", \"Latency (ms)\"],\n",
    "                  title=\"6Ô∏è‚É£ Performance vs. Efficiency Trade-off\",\n",
    "                  labels={\"BLEU Score\": \"BLEU Score (higher is better)\", \n",
    "                          \"Latency (ms)\": \"Latency in ms (lower is better)\"})\n",
    "\n",
    "# Add a trend line\n",
    "fig6.update_traces(marker=dict(line=dict(width=1, color='DarkSlateGrey')))\n",
    "fig6.update_layout(legend_title_text='Precision')\n",
    "fig6.write_html(\"visuals/interactive_performance_efficiency.html\")\n",
    "\n",
    "# ========== 7Ô∏è‚É£ Precision Degradation Analysis ==========\n",
    "# First, create a pivot table to compare the same model across different precisions\n",
    "pivot_df = combined.pivot_table(index=\"Model\", columns=\"Precision\", \n",
    "                              values=\"BLEU Score\", aggfunc=\"first\").reset_index()\n",
    "\n",
    "# Calculate degradation percentages\n",
    "for precision in [\"INT8\", \"INT4\"]:\n",
    "    pivot_df[f\"{precision} vs FP16 (%)\"] = ((pivot_df[precision] - pivot_df[\"FP16\"]) / pivot_df[\"FP16\"] * 100).round(2)\n",
    "\n",
    "# Melt the dataframe for visualization\n",
    "degradation_df = pivot_df.melt(id_vars=\"Model\", \n",
    "                              value_vars=[\"INT8 vs FP16 (%)\", \"INT4 vs FP16 (%)\"],\n",
    "                              var_name=\"Comparison\", value_name=\"Degradation (%)\")\n",
    "\n",
    "fig7 = px.bar(degradation_df, x=\"Model\", y=\"Degradation (%)\", color=\"Comparison\",\n",
    "             barmode=\"group\", title=\"7Ô∏è‚É£ BLEU Score Degradation Relative to FP16\",\n",
    "             labels={\"Degradation (%)\": \"% Change from FP16 (negative = worse)\"})\n",
    "\n",
    "fig7.update_layout(xaxis_tickangle=-45)\n",
    "fig7.write_html(\"visuals/interactive_precision_degradation.html\")\n",
    "\n",
    "# ========== 8Ô∏è‚É£ Model Size vs. Performance ==========\n",
    "# If you have model size data, you can add it to your combined dataframe\n",
    "# This is a placeholder - you'll need to add the actual model size data\n",
    "# Let's assume you have a dictionary mapping model names to their sizes in MB\n",
    "model_sizes = {\n",
    "    \"MODEL_A\": 350,\n",
    "    \"MODEL_B\": 420, \n",
    "    \"MODEL_C\": 500,\n",
    "    \"MODEL_D\": 650,\n",
    "    # Add all your models here\n",
    "}\n",
    "\n",
    "# Add model size to the combined dataframe\n",
    "combined[\"Model Size (MB)\"] = combined[\"Model\"].map(model_sizes)\n",
    "\n",
    "# Create a bubble chart\n",
    "fig8 = px.scatter(combined, x=\"Model Size (MB)\", y=\"BLEU Score\", \n",
    "                 size=\"Latency (ms)\", color=\"Precision\", symbol=\"Model\",\n",
    "                 hover_data=[\"Model\", \"Precision\", \"BLEU Score\"],\n",
    "                 title=\"8Ô∏è‚É£ Model Size vs. Performance Trade-off\",\n",
    "                 labels={\"BLEU Score\": \"BLEU Score\", \"Model Size (MB)\": \"Model Size (MB)\"})\n",
    "\n",
    "fig8.update_layout(xaxis_title=\"Model Size (MB)\")\n",
    "fig8.write_html(\"visuals/interactive_size_performance.html\")\n",
    "\n",
    "# ========== 9Ô∏è‚É£ Performance Distribution Boxplots ==========\n",
    "fig9 = px.box(combined, x=\"Precision\", y=\"BLEU Score\", color=\"Precision\",\n",
    "             points=\"all\", title=\"9Ô∏è‚É£ BLEU Score Distribution by Precision\",\n",
    "             labels={\"BLEU Score\": \"BLEU Score\", \"Precision\": \"Precision\"})\n",
    "\n",
    "fig9.write_html(\"visuals/interactive_performance_distribution.html\")\n",
    "\n",
    "# ========== üîü Performance Radar Charts ==========\n",
    "# This creates a radar chart to compare multiple metrics for each precision\n",
    "# Let's assume you have multiple metrics in your data\n",
    "# If not, you could use other columns or calculate additional metrics\n",
    "\n",
    "# Create a sample dataframe with multiple metrics\n",
    "# In a real scenario, you'd use your actual metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Precision\": [\"FP16\", \"INT8\", \"INT4\"],\n",
    "    \"BLEU Score\": avg_metrics[\"BLEU Score\"].tolist(),\n",
    "    \"Speed (1/Latency)\": (1000 / avg_metrics[\"Latency (ms)\"]).tolist(),\n",
    "    \"Memory Efficiency\": [1.0, 2.0, 4.0],  # Relative to FP16\n",
    "    \"Inference Throughput\": [1.0, 1.8, 3.5]  # Relative to FP16\n",
    "})\n",
    "\n",
    "# Normalize the metrics for better visualization\n",
    "for col in metrics_df.columns:\n",
    "    if col != \"Precision\":\n",
    "        max_val = metrics_df[col].max()\n",
    "        metrics_df[col] = metrics_df[col] / max_val\n",
    "\n",
    "# Create the radar chart\n",
    "metrics_melted = metrics_df.melt(id_vars=\"Precision\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "fig10 = px.line_polar(metrics_melted, r=\"Value\", theta=\"Metric\", color=\"Precision\", line_close=True,\n",
    "                     title=\"üîü Multi-metric Performance Comparison\",\n",
    "                     range_r=[0, 1])\n",
    "\n",
    "fig10.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1])))\n",
    "fig10.write_html(\"visuals/interactive_radar_chart.html\")\n",
    "\n",
    "# ========== 1Ô∏è‚É£1Ô∏è‚É£ Interactive Model Selector Dashboard ==========\n",
    "# Create a combined dashboard with model selector\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Function to create a model comparison dashboard\n",
    "def create_model_comparison(model_name):\n",
    "    model_data = combined[combined[\"Model\"] == model_name]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\"BLEU Score by Precision\", \"Latency by Precision\", \n",
    "                       \"BLEU vs Latency\", \"Precision Comparison\"),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "              [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # BLEU Score by Precision\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=model_data[\"Precision\"], y=model_data[\"BLEU Score\"], name=\"BLEU Score\",\n",
    "              marker_color=['#1f77b4', '#ff7f0e', '#2ca02c']),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Latency by Precision\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=model_data[\"Precision\"], y=model_data[\"Latency (ms)\"], name=\"Latency\",\n",
    "              marker_color=['#1f77b4', '#ff7f0e', '#2ca02c']),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # BLEU vs Latency Scatter\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=model_data[\"Latency (ms)\"], y=model_data[\"BLEU Score\"], mode=\"markers+text\",\n",
    "                  marker=dict(size=12, color=['#1f77b4', '#ff7f0e', '#2ca02c']),\n",
    "                  text=model_data[\"Precision\"], textposition=\"top center\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # If you have the precision degradation data\n",
    "    if \"MODEL_A\" in model_sizes:  # Just a check to ensure we have the previous code executed\n",
    "        model_pivot = pivot_df[pivot_df[\"Model\"] == model_name]\n",
    "        \n",
    "        if not model_pivot.empty:\n",
    "            degradation_data = {\n",
    "                \"Precision\": [\"FP16\", \"INT8\", \"INT4\"],\n",
    "                \"BLEU Score\": [\n",
    "                    model_pivot[\"FP16\"].values[0],\n",
    "                    model_pivot[\"INT8\"].values[0],\n",
    "                    model_pivot[\"INT4\"].values[0]\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            degradation_df = pd.DataFrame(degradation_data)\n",
    "            degradation_df[\"Relative\"] = degradation_df[\"BLEU Score\"] / degradation_df[\"BLEU Score\"].max()\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(x=degradation_df[\"Precision\"], y=degradation_df[\"Relative\"], \n",
    "                      name=\"Relative Score\", marker_color=['#1f77b4', '#ff7f0e', '#2ca02c'],\n",
    "                      text=[f\"{x:.2f}\" for x in degradation_df[\"BLEU Score\"]], textposition=\"auto\"),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(height=800, width=1000, \n",
    "                     title_text=f\"1Ô∏è‚É£1Ô∏è‚É£ Detailed Analysis for {model_name}\",\n",
    "                     showlegend=False)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create individual dashboard for each model\n",
    "for model in combined[\"Model\"].unique():\n",
    "    fig = create_model_comparison(model)\n",
    "    fig.write_html(f\"visuals/interactive_dashboard_{model}.html\")\n",
    "\n",
    "# Create an index for all model-specific dashboards\n",
    "models_list = combined[\"Model\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    \"Precision\": [\"FP16\", \"INT8\", \"INT4\"],\n",
    "    \"BLEU Score\": avg_metrics[\"BLEU Score\"].tolist(),\n",
    "    \"Speed (1/Latency)\": (1000 / avg_metrics[\"Latency (ms)\"]).tolist(),\n",
    "    \"Memory Efficiency\": [1.0, 2.0, 4.0],  # Customize if needed\n",
    "    \"Inference Throughput\": [1.0, 1.8, 3.5]\n",
    "})\n",
    "\n",
    "# Normalize metrics\n",
    "for col in metrics_df.columns[1:]:\n",
    "    metrics_df[col] = metrics_df[col] / metrics_df[col].max()\n",
    "\n",
    "metrics_melted = metrics_df.melt(id_vars=\"Precision\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "fig10 = px.line_polar(metrics_melted, r=\"Value\", theta=\"Metric\", color=\"Precision\", line_close=True,\n",
    "                     title=\"üîü Multi-metric Performance Comparison\",\n",
    "                     range_r=[0, 1])\n",
    "\n",
    "fig10.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1])))\n",
    "fig10.write_html(\"visuals/interactive_radar_chart.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 8Ô∏è‚É£ Model Size vs. Performance (Actual Implementation) ==========\n",
    "# Add this after your existing visualizations\n",
    "\n",
    "# Create a dictionary with actual model sizes in MB (you'll need to fill these in)\n",
    "model_sizes = {\n",
    "    \"qwen2.5-0.5b-instruct\": 500,\n",
    "    \"tiny-llama-1b-chat\": 1000,\n",
    "    \"DeepSeek-R1-Distill-Qwen-1.5B\": 1500,\n",
    "    \"DeepSeek-R1-Distill-Qwen-7B\": 7000,\n",
    "    \"qwen2.5-1.5b-instruct\": 1500,\n",
    "    \"gemma-2b-it\": 2000,\n",
    "    \"gemma-2-2b-it\": 2000,\n",
    "    \"qwen2.5-3b-instruct\": 3000,\n",
    "    \"minicpm3-4b\": 4000,\n",
    "    \"DeepSeek-R1-Distill-Llama-8B\": 8000,\n",
    "    \"qwen2.5-7b-instruct\": 7000,\n",
    "    \"llama-3.2-1b-instruct\": 1000,\n",
    "    \"llama-3.2-3b-instruct\": 3000,\n",
    "    \"zephyr-7b-beta\": 7000,\n",
    "    \"notus-7b-v1\": 7000,\n",
    "    \"gemma-2-9b-it\": 9000\n",
    "}\n",
    "\n",
    "# Add model size to the combined dataframe\n",
    "combined[\"Model Size (MB)\"] = combined[\"Model\"].map(model_sizes)\n",
    "\n",
    "# Create bubble chart with actual sizes\n",
    "fig8 = px.scatter(\n",
    "    combined, \n",
    "    x=\"Model Size (MB)\", \n",
    "    y=\"BLEU Score\", \n",
    "    size=\"Throughput (tokens/sec)\", \n",
    "    color=\"Precision\", \n",
    "    symbol=\"Model\",\n",
    "    hover_data=[\"Model\", \"Precision\", \"BLEU Score\", \"Latency (ms)\", \"Throughput (tokens/sec)\"],\n",
    "    title=\"8Ô∏è‚É£ Model Size vs. Performance Trade-off (Actual Sizes)\",\n",
    "    labels={\n",
    "        \"BLEU Score\": \"BLEU Score\", \n",
    "        \"Model Size (MB)\": \"Model Size (MB)\",\n",
    "        \"Throughput (tokens/sec)\": \"Throughput (tokens/sec)\"\n",
    "    }\n",
    ")\n",
    "\n",
    "fig8.update_layout(\n",
    "    xaxis_title=\"Model Size (MB) - Lower is Better\",\n",
    "    yaxis_title=\"BLEU Score - Higher is Better\",\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig8.write_html(\"visuals/interactive_size_performance_actual.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2Ô∏è‚É£2Ô∏è‚É£ Throughput vs. Latency Efficiency Frontier ==========\n",
    "# Add this after your existing visualizations\n",
    "\n",
    "fig22 = px.scatter(\n",
    "    combined,\n",
    "    x=\"Latency (ms)\",\n",
    "    y=\"Throughput (tokens/sec)\",\n",
    "    color=\"Model\",\n",
    "    symbol=\"Precision\",\n",
    "    size=\"BLEU Score\",\n",
    "    hover_data=[\"BLEU Score\", \"ROUGE-L\", \"CHRF Score\"],\n",
    "    title=\"2Ô∏è‚É£2Ô∏è‚É£ Throughput vs. Latency Efficiency Frontier\",\n",
    "    labels={\n",
    "        \"Latency (ms)\": \"Latency (ms) - Lower is Better\",\n",
    "        \"Throughput (tokens/sec)\": \"Throughput (tokens/sec) - Higher is Better\",\n",
    "        \"BLEU Score\": \"BLEU Score\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add trend lines for each model\n",
    "for model in combined[\"Model\"].unique():\n",
    "    model_data = combined[combined[\"Model\"] == model]\n",
    "    if len(model_data) > 1:  # Only add trend line if multiple points exist\n",
    "        fig22.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_data[\"Latency (ms)\"],\n",
    "                y=model_data[\"Throughput (tokens/sec)\"],\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=1, dash=\"dot\"),\n",
    "                name=f\"{model} Trend\",\n",
    "                showlegend=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "fig22.update_layout(\n",
    "    height=700,\n",
    "    xaxis_type=\"log\",  # Use log scale for better visualization\n",
    "    yaxis_type=\"log\"\n",
    ")\n",
    "\n",
    "fig22.write_html(\"visuals/interactive_throughput_latency_frontier.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2Ô∏è‚É£3Ô∏è‚É£ Precision Impact on Quality Metrics ==========\n",
    "# Add this after your existing visualizations\n",
    "\n",
    "# Melt the dataframe for quality metrics\n",
    "quality_metrics = [\"BLEU Score\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"CHRF Score\"]\n",
    "melted_quality = combined.melt(\n",
    "    id_vars=[\"Model\", \"Precision\"], \n",
    "    value_vars=quality_metrics,\n",
    "    var_name=\"Metric\", \n",
    "    value_name=\"Score\"\n",
    ")\n",
    "\n",
    "fig23 = px.box(\n",
    "    melted_quality,\n",
    "    x=\"Precision\",\n",
    "    y=\"Score\",\n",
    "    color=\"Precision\",\n",
    "    facet_col=\"Metric\",\n",
    "    facet_col_wrap=3,\n",
    "    title=\"2Ô∏è‚É£3Ô∏è‚É£ Impact of Precision on Different Quality Metrics\",\n",
    "    labels={\n",
    "        \"Score\": \"Metric Value\",\n",
    "        \"Precision\": \"Precision Level\"\n",
    "    }\n",
    ")\n",
    "\n",
    "fig23.update_layout(\n",
    "    height=800,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig23.update_yaxes(matches=None)  # Allow different y-axis scales per metric\n",
    "fig23.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "\n",
    "fig23.write_html(\"visuals/interactive_precision_impact_quality.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2Ô∏è‚É£4Ô∏è‚É£ Model Ranking by Multiple Metrics ==========\n",
    "# Add this after your existing visualizations\n",
    "\n",
    "# Calculate normalized scores (0-1) for each metric\n",
    "metrics_to_rank = {\n",
    "    \"BLEU Score\": True,  # Higher is better\n",
    "    \"ROUGE-L\": True,\n",
    "    \"CHRF Score\": True,\n",
    "    \"Latency (ms)\": False,  # Lower is better\n",
    "    \"Throughput (tokens/sec)\": True\n",
    "}\n",
    "\n",
    "# Create a ranking dataframe\n",
    "ranking_df = combined.copy()\n",
    "for metric, higher_better in metrics_to_rank.items():\n",
    "    if higher_better:\n",
    "        ranking_df[f\"{metric}_norm\"] = (ranking_df[metric] - ranking_df[metric].min()) / (ranking_df[metric].max() - ranking_df[metric].min())\n",
    "    else:\n",
    "        ranking_df[f\"{metric}_norm\"] = 1 - (ranking_df[metric] - ranking_df[metric].min()) / (ranking_df[metric].max() - ranking_df[metric].min())\n",
    "\n",
    "# Calculate composite score (weighted average)\n",
    "weights = {\n",
    "    \"BLEU Score_norm\": 0.3,\n",
    "    \"ROUGE-L_norm\": 0.2,\n",
    "    \"CHRF Score_norm\": 0.2,\n",
    "    \"Latency (ms)_norm\": 0.15,\n",
    "    \"Throughput (tokens/sec)_norm\": 0.15\n",
    "}\n",
    "\n",
    "ranking_df[\"Composite Score\"] = sum(ranking_df[col] * weight for col, weight in weights.items())\n",
    "\n",
    "# Get top models by composite score\n",
    "top_models = ranking_df.sort_values(\"Composite Score\", ascending=False).drop_duplicates([\"Model\", \"Precision\"])\n",
    "\n",
    "# Create ranking visualization\n",
    "fig24 = px.bar(\n",
    "    top_models.sort_values(\"Composite Score\", ascending=True),\n",
    "    x=\"Composite Score\",\n",
    "    y=\"variant_id\",\n",
    "    color=\"Precision\",\n",
    "    orientation=\"h\",\n",
    "    hover_data=list(metrics_to_rank.keys()),\n",
    "    title=\"2Ô∏è‚É£4Ô∏è‚É£ Model Ranking by Composite Score (Higher is Better)\",\n",
    "    labels={\n",
    "        \"Composite Score\": \"Composite Score (0-1)\",\n",
    "        \"variant_id\": \"Model + Precision\"\n",
    "    }\n",
    ")\n",
    "\n",
    "fig24.update_layout(\n",
    "    height=900,\n",
    "    yaxis={\"categoryorder\": \"total ascending\"}\n",
    ")\n",
    "\n",
    "fig24.write_html(\"visuals/interactive_model_ranking_composite.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2Ô∏è‚É£5Ô∏è‚É£ Performance per Parameter Count ==========\n",
    "# Add this after your existing visualizations\n",
    "\n",
    "# Add parameter counts to your dataframe (you'll need to fill these in)\n",
    "param_counts = {\n",
    "    \"qwen2.5-0.5b-instruct\": 0.5,\n",
    "    \"tiny-llama-1b-chat\": 1.0,\n",
    "    \"DeepSeek-R1-Distill-Qwen-1.5B\": 1.5,\n",
    "    \"DeepSeek-R1-Distill-Qwen-7B\": 7.0,\n",
    "    \"qwen2.5-1.5b-instruct\": 1.5,\n",
    "    \"gemma-2b-it\": 2.0,\n",
    "    \"gemma-2-2b-it\": 2.0,\n",
    "    \"qwen2.5-3b-instruct\": 3.0,\n",
    "    \"minicpm3-4b\": 4.0,\n",
    "    \"DeepSeek-R1-Distill-Llama-8B\": 8.0,\n",
    "    \"qwen2.5-7b-instruct\": 7.0,\n",
    "    \"llama-3.2-1b-instruct\": 1.0,\n",
    "    \"llama-3.2-3b-instruct\": 3.0,\n",
    "    \"zephyr-7b-beta\": 7.0,\n",
    "    \"notus-7b-v1\": 7.0,\n",
    "    \"gemma-2-9b-it\": 9.0\n",
    "}\n",
    "\n",
    "combined[\"Parameter Count (B)\"] = combined[\"Model\"].map(param_counts)\n",
    "combined[\"BLEU per Billion Params\"] = combined[\"BLEU Score\"] / combined[\"Parameter Count (B)\"]\n",
    "combined[\"Throughput per Billion Params\"] = combined[\"Throughput (tokens/sec)\"] / combined[\"Parameter Count (B)\"]\n",
    "\n",
    "fig25 = px.scatter(\n",
    "    combined,\n",
    "    x=\"Parameter Count (B)\",\n",
    "    y=\"BLEU per Billion Params\",\n",
    "    color=\"Precision\",\n",
    "    symbol=\"Model\",\n",
    "    size=\"Throughput per Billion Params\",\n",
    "    hover_data=[\"BLEU Score\", \"Throughput (tokens/sec)\", \"Latency (ms)\"],\n",
    "    title=\"2Ô∏è‚É£5Ô∏è‚É£ Performance Efficiency per Billion Parameters\",\n",
    "    labels={\n",
    "        \"Parameter Count (B)\": \"Model Size (Billion Parameters)\",\n",
    "        \"BLEU per Billion Params\": \"BLEU Score per Billion Parameters\",\n",
    "        \"Throughput per Billion Params\": \"Throughput per Billion Params\"\n",
    "    }\n",
    ")\n",
    "\n",
    "fig25.update_layout(\n",
    "    height=700,\n",
    "    xaxis_title=\"Model Size (Billion Parameters)\",\n",
    "    yaxis_title=\"BLEU Score per Billion Parameters (Higher is Better)\"\n",
    ")\n",
    "\n",
    "fig25.write_html(\"visuals/interactive_performance_per_parameter.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2Ô∏è‚É£6Ô∏è‚É£ Model Family Performance Comparison ==========\n",
    "# Extract model family from names\n",
    "combined['Model Family'] = combined['Model'].str.extract(r'^([a-zA-Z\\-]+)')[0]\n",
    "\n",
    "fig26 = px.box(\n",
    "    combined,\n",
    "    x=\"Model Family\",\n",
    "    y=\"BLEU Score\",\n",
    "    color=\"Precision\",\n",
    "    title=\"2Ô∏è‚É£6Ô∏è‚É£ Performance Comparison by Model Family\",\n",
    "    labels={\"BLEU Score\": \"BLEU Score\", \"Model Family\": \"Model Family\"}\n",
    ")\n",
    "fig26.update_layout(xaxis_tickangle=-45)\n",
    "fig26.write_html(\"visuals/interactive_model_family_comparison.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2Ô∏è‚É£7Ô∏è‚É£ Speed-Quality Trade-off Matrix ==========\n",
    "fig27 = px.scatter(\n",
    "    combined,\n",
    "    x=\"Latency (ms)\",\n",
    "    y=\"BLEU Score\",\n",
    "    color=\"Throughput (tokens/sec)\",\n",
    "    size=\"Model Size (MB)\",\n",
    "    facet_col=\"Precision\",\n",
    "    hover_data=[\"Model\", \"ROUGE-L\", \"CHRF Score\"],\n",
    "    title=\"2Ô∏è‚É£7Ô∏è‚É£ Speed-Quality Trade-off Matrix by Precision\",\n",
    "    labels={\n",
    "        \"Latency (ms)\": \"Latency (ms) - Lower is Better\",\n",
    "        \"BLEU Score\": \"BLEU Score - Higher is Better\",\n",
    "        \"Throughput (tokens/sec)\": \"Throughput\"\n",
    "    }\n",
    ")\n",
    "fig27.update_layout(height=500)\n",
    "fig27.write_html(\"visuals/interactive_speed_quality_matrix.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2Ô∏è‚É£8Ô∏è‚É£ Precision Degradation by Model Size ==========\n",
    "# Calculate degradation percentages\n",
    "pivot_df = combined.pivot_table(index=[\"Model\", \"Model Size (MB)\"], \n",
    "                              columns=\"Precision\", values=\"BLEU Score\").reset_index()\n",
    "for precision in [\"INT8\", \"INT4\"]:\n",
    "    pivot_df[f\"{precision}_degradation\"] = ((pivot_df[precision] - pivot_df[\"FP16\"]) / pivot_df[\"FP16\"] * 100)\n",
    "\n",
    "fig28 = px.scatter(\n",
    "    pivot_df,\n",
    "    x=\"Model Size (MB)\",\n",
    "    y=\"INT8_degradation\",\n",
    "    size=\"FP16\",\n",
    "    color=\"Model\",\n",
    "    trendline=\"lowess\",\n",
    "    title=\"2Ô∏è‚É£8Ô∏è‚É£ INT8 Precision Degradation by Model Size\",\n",
    "    labels={\n",
    "        \"Model Size (MB)\": \"Model Size (MB)\",\n",
    "        \"INT8_degradation\": \"INT8 BLEU Score Degradation (%)\",\n",
    "        \"FP16\": \"FP16 BLEU Score\"\n",
    "    }\n",
    ")\n",
    "fig28.write_html(\"visuals/interactive_degradation_by_size.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3Ô∏è‚É£0Ô∏è‚É£ Model Efficiency Score ==========\n",
    "# Calculate composite efficiency score\n",
    "combined[\"Efficiency Score\"] = (\n",
    "    0.4 * (1 - (combined[\"Latency (ms)\"] - combined[\"Latency (ms)\"].min()) / \n",
    "           (combined[\"Latency (ms)\"].max() - combined[\"Latency (ms)\"].min())) +\n",
    "    0.4 * ((combined[\"Throughput (tokens/sec)\"] - combined[\"Throughput (tokens/sec)\"].min()) / \n",
    "           (combined[\"Throughput (tokens/sec)\"].max() - combined[\"Throughput (tokens/sec)\"].min())) +\n",
    "    0.2 * (combined[\"BLEU Score\"] - combined[\"BLEU Score\"].min()) / \n",
    "           (combined[\"BLEU Score\"].max() - combined[\"BLEU Score\"].min())\n",
    ")\n",
    "\n",
    "fig30 = px.bar(\n",
    "    combined.sort_values(\"Efficiency Score\", ascending=False),\n",
    "    x=\"variant_id\",\n",
    "    y=\"Efficiency Score\",\n",
    "    color=\"Precision\",\n",
    "    title=\"3Ô∏è‚É£0Ô∏è‚É£ Model Efficiency Score (Higher is Better)\",\n",
    "    labels={\"variant_id\": \"Model + Precision\", \"Efficiency Score\": \"Efficiency Score (0-1)\"}\n",
    ")\n",
    "fig30.update_layout(xaxis_tickangle=-90, height=600)\n",
    "fig30.write_html(\"visuals/interactive_efficiency_score.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3Ô∏è‚É£1Ô∏è‚É£ Performance vs. Model Size Growth ==========\n",
    "fig31 = px.scatter(\n",
    "    combined,\n",
    "    x=\"Model Size (MB)\",\n",
    "    y=\"BLEU Score\",\n",
    "    animation_frame=\"Precision\",\n",
    "    animation_group=\"Model\",\n",
    "    size=\"Throughput (tokens/sec)\",\n",
    "    color=\"Model Family\",\n",
    "    hover_name=\"Model\",\n",
    "    title=\"3Ô∏è‚É£1Ô∏è‚É£ Performance vs. Model Size Growth by Precision\",\n",
    "    labels={\n",
    "        \"Model Size (MB)\": \"Model Size (MB)\",\n",
    "        \"BLEU Score\": \"BLEU Score\",\n",
    "        \"Throughput (tokens/sec)\": \"Throughput\"\n",
    "    }\n",
    ")\n",
    "fig31.update_layout(height=700)\n",
    "fig31.write_html(\"visuals/interactive_size_growth.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = combined[[\"BLEU Score\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"CHRF Score\"]].corr()\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig32 = go.Figure(go.Sankey(\n",
    "    node=dict(\n",
    "        label=[\"BLEU\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"CHRF\"],\n",
    "        color=\"blue\"\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=[0, 0, 0, 0, 1, 1, 1, 2, 2, 3],\n",
    "        target=[1, 2, 3, 4, 2, 3, 4, 3, 4, 4],\n",
    "        value=[\n",
    "            abs(corr_matrix.iloc[0,1]*10),  # BLEU -> ROUGE-1\n",
    "            abs(corr_matrix.iloc[0,2]*10),  # BLEU -> ROUGE-2\n",
    "            abs(corr_matrix.iloc[0,3]*10),  # BLEU -> ROUGE-L\n",
    "            abs(corr_matrix.iloc[0,4]*10),  # BLEU -> CHRF\n",
    "            abs(corr_matrix.iloc[1,2]*10),  # ROUGE-1 -> ROUGE-2\n",
    "            abs(corr_matrix.iloc[1,3]*10),  # ROUGE-1 -> ROUGE-L\n",
    "            abs(corr_matrix.iloc[1,4]*10),  # ROUGE-1 -> CHRF\n",
    "            abs(corr_matrix.iloc[2,3]*10),  # ROUGE-2 -> ROUGE-L\n",
    "            abs(corr_matrix.iloc[2,4]*10),  # ROUGE-2 -> CHRF\n",
    "            abs(corr_matrix.iloc[3,4]*10)   # ROUGE-L -> CHRF\n",
    "        ]\n",
    "    )\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig32.update_layout(\n",
    "    title_text=\"3Ô∏è‚É£2Ô∏è‚É£ Quality Metrics Correlation Network\",\n",
    "    font_size=12,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Save the visualization\n",
    "fig32.write_html(\"visuals/interactive_metrics_network.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3Ô∏è‚É£3Ô∏è‚É£ Precision Impact on Latency/Throughput ==========\n",
    "fig33 = make_subplots(rows=1, cols=2, subplot_titles=(\"Latency Impact\", \"Throughput Impact\"))\n",
    "\n",
    "for i, metric in enumerate([\"Latency (ms)\", \"Throughput (tokens/sec)\"]):\n",
    "    fig33.add_trace(\n",
    "        go.Box(\n",
    "            x=combined[\"Precision\"],\n",
    "            y=combined[metric],\n",
    "            name=metric\n",
    "        ),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "\n",
    "fig33.update_layout(\n",
    "    title_text=\"3Ô∏è‚É£3Ô∏è‚É£ Precision Impact on Latency and Throughput\",\n",
    "    showlegend=False,\n",
    "    height=500\n",
    ")\n",
    "fig33.write_html(\"visuals/interactive_precision_impact_latency_throughput.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics of interest\n",
    "metrics = [\"BLEU Score\", \"ROUGE-L\", \"CHRF Score\", \"Latency (ms)\", \"Throughput (tokens/sec)\"]\n",
    "best_models = pd.DataFrame()\n",
    "\n",
    "# Find the best model for each metric and precision\n",
    "for metric in metrics:\n",
    "    if metric == \"Latency (ms)\":\n",
    "        best = combined.loc[combined.groupby(\"Precision\")[metric].idxmin()]\n",
    "    else:\n",
    "        best = combined.loc[combined.groupby(\"Precision\")[metric].idxmax()]\n",
    "    \n",
    "    best = best.copy()\n",
    "    best[\"Metric\"] = metric\n",
    "    best[\"value\"] = best[metric]\n",
    "    best_models = pd.concat([best_models, best], ignore_index=True)\n",
    "\n",
    "# Plot the results using Plotly\n",
    "fig35 = px.bar(\n",
    "    best_models,\n",
    "    x=\"Precision\",\n",
    "    y=\"value\",\n",
    "    facet_col=\"Metric\",\n",
    "    facet_col_wrap=3,\n",
    "    color=\"Model\",\n",
    "    title=\"3Ô∏è‚É£5Ô∏è‚É£ Best Model by Metric and Precision\",\n",
    "    labels={\"value\": \"Metric Value\"}\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig35.update_layout(height=800)\n",
    "\n",
    "# Save the interactive chart\n",
    "fig35.write_html(\"visuals/interactive_best_by_metric.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1Ô∏è‚É£ CHRF Score Comparison ==========\n",
    "fig_chrf = px.box(\n",
    "    combined,\n",
    "    x=\"Precision\",\n",
    "    y=\"CHRF Score\",\n",
    "    color=\"Precision\",\n",
    "    title=\"CHRF Score Comparison Across Precisions\",\n",
    "    labels={\"CHRF Score\": \"CHRF Score\", \"Precision\": \"Precision\"}\n",
    ")\n",
    "fig_chrf.update_layout(height=500)\n",
    "fig_chrf.write_html(\"visuals/interactive_chrf_comparison.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2Ô∏è‚É£ ROUGE-L Comparison ==========\n",
    "fig_rouge_l = px.violin(\n",
    "    combined,\n",
    "    x=\"Precision\",\n",
    "    y=\"ROUGE-L\",\n",
    "    color=\"Precision\",\n",
    "    box=True,\n",
    "    points=\"all\",\n",
    "    title=\"ROUGE-L Comparison Across Precisions\",\n",
    "    labels={\"ROUGE-L\": \"ROUGE-L Score\", \"Precision\": \"Precision\"}\n",
    ")\n",
    "fig_rouge_l.update_layout(height=500)\n",
    "fig_rouge_l.write_html(\"visuals/interactive_rouge_l_comparison.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3Ô∏è‚É£ ROUGE-1 Comparison ==========\n",
    "fig_rouge_1 = px.strip(\n",
    "    combined,\n",
    "    x=\"Precision\",\n",
    "    y=\"ROUGE-1\",\n",
    "    color=\"Model\",\n",
    "    facet_col=\"Model Family\",\n",
    "    facet_col_wrap=4,\n",
    "    title=\"ROUGE-1 Comparison Across Precisions by Model Family\",\n",
    "    labels={\"ROUGE-1\": \"ROUGE-1 Score\", \"Precision\": \"Precision\"}\n",
    ")\n",
    "fig_rouge_1.update_layout(height=700, showlegend=False)\n",
    "fig_rouge_1.write_html(\"visuals/interactive_rouge_1_comparison.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 4Ô∏è‚É£ ROUGE-2 Comparison ==========\n",
    "fig_rouge_2 = px.bar(\n",
    "    combined,\n",
    "    x=\"Model\",\n",
    "    y=\"ROUGE-2\",\n",
    "    color=\"Precision\",\n",
    "    barmode=\"group\",\n",
    "    title=\"ROUGE-2 Comparison Across Precisions by Model\",\n",
    "    labels={\"ROUGE-2\": \"ROUGE-2 Score\", \"Model\": \"Model\"}\n",
    ")\n",
    "fig_rouge_2.update_layout(xaxis_tickangle=-45, height=600)\n",
    "fig_rouge_2.write_html(\"visuals/interactive_rouge_2_comparison.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 4Ô∏è‚É£ ROUGE-2 Comparison ==========\n",
    "fig_rouge_2 = px.bar(\n",
    "    combined,\n",
    "    x=\"Model\",\n",
    "    y=\"ROUGE-2\",\n",
    "    color=\"Precision\",\n",
    "    barmode=\"group\",\n",
    "    title=\"ROUGE-2 Comparison Across Precisions by Model\",\n",
    "    labels={\"ROUGE-2\": \"ROUGE-2 Score\", \"Model\": \"Model\"}\n",
    ")\n",
    "fig_rouge_2.update_layout(xaxis_tickangle=-45, height=600)\n",
    "fig_rouge_2.write_html(\"visuals/interactive_rouge_2_comparison.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 5Ô∏è‚É£ All Variants CHRF ==========\n",
    "fig_all_chrf = px.line(\n",
    "    combined,\n",
    "    x=\"variant_id\",\n",
    "    y=\"CHRF Score\",\n",
    "    color=\"Precision\",\n",
    "    title=\"CHRF Scores Across All Model Variants\",\n",
    "    labels={\"variant_id\": \"Model + Precision\", \"CHRF Score\": \"CHRF Score\"}\n",
    ")\n",
    "fig_all_chrf.update_layout(xaxis_tickangle=-90, height=600)\n",
    "fig_all_chrf.write_html(\"visuals/interactive_all_variants_chrf.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 6Ô∏è‚É£ All Variants ROUGE-L ==========\n",
    "fig_all_rouge_l = px.scatter(\n",
    "    combined,\n",
    "    x=\"variant_id\",\n",
    "    y=\"ROUGE-L\",\n",
    "    color=\"Precision\",\n",
    "    size=\"BLEU Score\",\n",
    "    title=\"ROUGE-L Scores Across All Model Variants\",\n",
    "    labels={\"variant_id\": \"Model + Precision\", \"ROUGE-L\": \"ROUGE-L Score\"}\n",
    ")\n",
    "fig_all_rouge_l.update_layout(xaxis_tickangle=-90, height=600)\n",
    "fig_all_rouge_l.write_html(\"visuals/interactive_all_variants_rouge_l.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 7Ô∏è‚É£ All Variants ROUGE-1 ==========\n",
    "fig_all_rouge_1 = px.bar(\n",
    "    combined.sort_values(\"ROUGE-1\", ascending=False),\n",
    "    x=\"variant_id\",\n",
    "    y=\"ROUGE-1\",\n",
    "    color=\"Precision\",\n",
    "    title=\"ROUGE-1 Scores Across All Model Variants (Sorted)\",\n",
    "    labels={\"variant_id\": \"Model + Precision\", \"ROUGE-1\": \"ROUGE-1 Score\"}\n",
    ")\n",
    "fig_all_rouge_1.update_layout(xaxis_tickangle=-90, height=600)\n",
    "fig_all_rouge_1.write_html(\"visuals/interactive_all_variants_rouge_1.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 8Ô∏è‚É£ All Variants ROUGE-2 ==========\n",
    "fig_all_rouge_2 = px.scatter(\n",
    "    combined,\n",
    "    x=\"Model Size (MB)\",\n",
    "    y=\"ROUGE-2\",\n",
    "    color=\"Precision\",\n",
    "    symbol=\"Model Family\",\n",
    "    size=\"Throughput (tokens/sec)\",\n",
    "    title=\"ROUGE-2 Scores vs Model Size\",\n",
    "    labels={\n",
    "        \"Model Size (MB)\": \"Model Size (MB)\",\n",
    "        \"ROUGE-2\": \"ROUGE-2 Score\",\n",
    "        \"Model Family\": \"Model Family\"\n",
    "    }\n",
    ")\n",
    "fig_all_rouge_2.update_layout(height=600)\n",
    "fig_all_rouge_2.write_html(\"visuals/interactive_all_variants_rouge_2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== üîü CHRF Degradation ==========\n",
    "pivot_df = combined.pivot_table(index=\"Model\", columns=\"Precision\", values=\"CHRF Score\").reset_index()\n",
    "for precision in [\"INT8\", \"INT4\"]:\n",
    "    pivot_df[f\"{precision}_degradation\"] = ((pivot_df[precision] - pivot_df[\"FP16\"]) / pivot_df[\"FP16\"] * 100)\n",
    "\n",
    "fig_chrf_degradation = px.bar(\n",
    "    pivot_df.melt(id_vars=\"Model\", \n",
    "                value_vars=[\"INT8_degradation\", \"INT4_degradation\"],\n",
    "                var_name=\"Comparison\", \n",
    "                value_name=\"Degradation (%)\"),\n",
    "    x=\"Model\",\n",
    "    y=\"Degradation (%)\",\n",
    "    color=\"Comparison\",\n",
    "    barmode=\"group\",\n",
    "    title=\"CHRF Score Degradation Relative to FP16\",\n",
    "    labels={\"Degradation (%)\": \"% Change from FP16 (negative = worse)\"}\n",
    ")\n",
    "fig_chrf_degradation.update_layout(xaxis_tickangle=-45, height=600)\n",
    "fig_chrf_degradation.write_html(\"visuals/interactive_chrf_degradation.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1Ô∏è‚É£1Ô∏è‚É£ ROUGE-L Degradation ==========\n",
    "pivot_df = combined.pivot_table(index=\"Model\", columns=\"Precision\", values=\"ROUGE-L\").reset_index()\n",
    "for precision in [\"INT8\", \"INT4\"]:\n",
    "    pivot_df[f\"{precision}_degradation\"] = ((pivot_df[precision] - pivot_df[\"FP16\"]) / pivot_df[\"FP16\"] * 100)\n",
    "\n",
    "fig_rouge_l_degradation = px.bar(\n",
    "    pivot_df.melt(id_vars=\"Model\", \n",
    "                value_vars=[\"INT8_degradation\", \"INT4_degradation\"],\n",
    "                var_name=\"Comparison\", \n",
    "                value_name=\"Degradation (%)\"),\n",
    "    x=\"Model\",\n",
    "    y=\"Degradation (%)\",\n",
    "    color=\"Comparison\",\n",
    "    barmode=\"group\",\n",
    "    title=\"ROUGE-L Score Degradation Relative to FP16\",\n",
    "    labels={\"Degradation (%)\": \"% Change from FP16 (negative = worse)\"}\n",
    ")\n",
    "fig_rouge_l_degradation.update_layout(xaxis_tickangle=-45, height=600)\n",
    "fig_rouge_l_degradation.write_html(\"visuals/interactive_rouge_l_degradation.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1Ô∏è‚É£2Ô∏è‚É£ ROUGE-1 Degradation ==========\n",
    "pivot_df = combined.pivot_table(index=\"Model\", columns=\"Precision\", values=\"ROUGE-1\").reset_index()\n",
    "for precision in [\"INT8\", \"INT4\"]:\n",
    "    pivot_df[f\"{precision}_degradation\"] = ((pivot_df[precision] - pivot_df[\"FP16\"]) / pivot_df[\"FP16\"] * 100)\n",
    "\n",
    "fig_rouge_1_degradation = px.bar(\n",
    "    pivot_df.melt(id_vars=\"Model\", \n",
    "                value_vars=[\"INT8_degradation\", \"INT4_degradation\"],\n",
    "                var_name=\"Comparison\", \n",
    "                value_name=\"Degradation (%)\"),\n",
    "    x=\"Model\",\n",
    "    y=\"Degradation (%)\",\n",
    "    color=\"Comparison\",\n",
    "    barmode=\"group\",\n",
    "    title=\"ROUGE-1 Score Degradation Relative to FP16\",\n",
    "    labels={\"Degradation (%)\": \"% Change from FP16 (negative = worse)\"}\n",
    ")\n",
    "fig_rouge_1_degradation.update_layout(xaxis_tickangle=-45, height=600)\n",
    "fig_rouge_1_degradation.write_html(\"visuals/interactive_rouge_1_degradation.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1Ô∏è‚É£3Ô∏è‚É£ ROUGE-2 Degradation ==========\n",
    "pivot_df = combined.pivot_table(index=\"Model\", columns=\"Precision\", values=\"ROUGE-2\").reset_index()\n",
    "for precision in [\"INT8\", \"INT4\"]:\n",
    "    pivot_df[f\"{precision}_degradation\"] = ((pivot_df[precision] - pivot_df[\"FP16\"]) / pivot_df[\"FP16\"] * 100)\n",
    "\n",
    "fig_rouge_2_degradation = px.bar(\n",
    "    pivot_df.melt(id_vars=\"Model\", \n",
    "                value_vars=[\"INT8_degradation\", \"INT4_degradation\"],\n",
    "                var_name=\"Comparison\", \n",
    "                value_name=\"Degradation (%)\"),\n",
    "    x=\"Model\",\n",
    "    y=\"Degradation (%)\",\n",
    "    color=\"Comparison\",\n",
    "    barmode=\"group\",\n",
    "    title=\"ROUGE-2 Score Degradation Relative to FP16\",\n",
    "    labels={\"Degradation (%)\": \"% Change from FP16 (negative = worse)\"}\n",
    ")\n",
    "fig_rouge_2_degradation.update_layout(xaxis_tickangle=-45, height=600)\n",
    "fig_rouge_2_degradation.write_html(\"visuals/interactive_rouge_2_degradation.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1Ô∏è‚É£4Ô∏è‚É£ Improved Performance vs Efficiency ==========\n",
    "fig_perf_eff = px.scatter(\n",
    "    combined,\n",
    "    x=\"Latency (ms)\",\n",
    "    y=\"BLEU Score\",\n",
    "    color=\"Model Family\",\n",
    "    symbol=\"Precision\",\n",
    "    size=\"Throughput (tokens/sec)\",\n",
    "    facet_col=\"Precision\",\n",
    "    hover_data=[\"Model\", \"ROUGE-L\", \"CHRF Score\"],\n",
    "    title=\"Performance vs Efficiency by Precision\",\n",
    "    labels={\n",
    "        \"Latency (ms)\": \"Latency (ms) - Lower is Better\",\n",
    "        \"BLEU Score\": \"BLEU Score - Higher is Better\",\n",
    "        \"Throughput (tokens/sec)\": \"Throughput\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add trend lines for each model family\n",
    "for family in combined[\"Model Family\"].unique():\n",
    "    for precision in combined[\"Precision\"].unique():\n",
    "        subset = combined[(combined[\"Model Family\"] == family) & (combined[\"Precision\"] == precision)]\n",
    "        if len(subset) > 1:\n",
    "            fig_perf_eff.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=subset[\"Latency (ms)\"],\n",
    "                    y=subset[\"BLEU Score\"],\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(width=1, dash=\"dot\"),\n",
    "                    name=f\"{family} Trend\",\n",
    "                    showlegend=False,\n",
    "                    legendgroup=family\n",
    "                ),\n",
    "                row=1, col=[\"FP16\", \"INT8\", \"INT4\"].index(precision)+1\n",
    "            )\n",
    "\n",
    "fig_perf_eff.update_layout(\n",
    "    height=500,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=-0.3,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5\n",
    "    )\n",
    ")\n",
    "fig_perf_eff.write_html(\"visuals/interactive_performance_efficiency_improved.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1Ô∏è‚É£5Ô∏è‚É£ Entropy Comparison ==========\n",
    "fig_entropy = px.box(\n",
    "    combined,\n",
    "    x=\"Model Family\",\n",
    "    y=\"Entropy\",\n",
    "    color=\"Precision\",\n",
    "    points=\"all\",\n",
    "    title=\"Output Entropy Comparison by Model Family and Precision\",\n",
    "    labels={\n",
    "        \"Entropy\": \"Output Entropy (Higher = More Diverse)\",\n",
    "        \"Model Family\": \"Model Family\"\n",
    "    }\n",
    ")\n",
    "\n",
    "fig_entropy.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    height=600,\n",
    "    yaxis_title=\"Output Entropy (Higher = More Diverse)\"\n",
    ")\n",
    "fig_entropy.write_html(\"visuals/interactive_entropy_comparison.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
